"""
RAGç³»ç»Ÿæ ¸å¿ƒå®ç°
ä½¿ç”¨LlamaIndex + ChromaDB
"""

from typing import List, Dict, Optional
from pathlib import Path
import os

# LlamaIndexæ ¸å¿ƒ
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.llms import LLM
from llama_index.core import StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore

# ChromaDB
import chromadb

# åµŒå…¥æ¨¡å‹
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# LLMï¼ˆé€šè¿‡Ollamaï¼‰
from llama_index.llms.ollama import Ollama


class NothingToAddRAG:
    """Nothing to Addé¡¹ç›®çš„RAGç³»ç»Ÿ"""

    def __init__(
        self,
        data_dir: str = "./data/processed",
        persist_dir: str = "./chroma_db",
        embed_model_name: str = "BAAI/bge-small-en-v1.5",  # æˆ–ç”¨ä¸­æ–‡æ¨¡å‹
        llm_model: str = "llama3.1",  # æˆ– "qwen2.5"
        chunk_size: int = 512,
        chunk_overlap: int = 50,
    ):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ

        Args:
            data_dir: æ•°æ®ç›®å½•
            persist_dir: ChromaDBæŒä¹…åŒ–ç›®å½•
            embed_model_name: åµŒå…¥æ¨¡å‹åç§°
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆOllamaï¼‰
            chunk_size: æ–‡æœ¬åˆ†å‰²å¤§å°
            chunk_overlap: æ–‡æœ¬é‡å å¤§å°
        """
        self.data_dir = Path(data_dir)
        self.persist_dir = Path(persist_dir)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.persist_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {embed_model_name}")
        self.embed_model = HuggingFaceEmbedding(
            model_name=embed_model_name,
            device="cpu"  # æˆ– "cuda" å¦‚æœæœ‰GPU
        )

        # åˆå§‹åŒ–LLM
        print(f"ğŸ¤– åˆå§‹åŒ–LLM: {llm_model}")
        self.llm = Ollama(
            model=llm_model,
            request_timeout=120.0
        )

        # åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“
        self.index = self._load_or_create_index()

    def _load_or_create_index(self) -> VectorStoreIndex:
        """åŠ è½½æˆ–åˆ›å»ºå‘é‡ç´¢å¼•"""

        # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
        chroma_client = chromadb.PersistentClient(
            path=str(self.persist_dir)
        )

        # åˆ›å»ºcollection
        collection = chroma_client.get_or_create_collection(
            name="nothing_to_add"
        )

        # åˆ›å»ºvector store
        vector_store = ChromaVectorStore(
            chroma_collection=collection
        )

        # åˆ›å»ºstorage context
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store
        )

        # å°è¯•åŠ è½½ç°æœ‰æ•°æ®
        if collection.count() > 0:
            print(f"âœ… åŠ è½½ç°æœ‰ç´¢å¼• ({collection.count()} ä¸ªæ–‡æ¡£)")
            index = VectorStoreIndex.from_vector_store(
                vector_store=vector_store,
                storage_context=storage_context,
                embed_model=self.embed_model
            )
            return index

        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºæ–°ç´¢å¼•
        print("ğŸ“š åˆ›å»ºæ–°ç´¢å¼•...")
        documents = self._load_documents()

        if not documents:
            print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·å…ˆæ·»åŠ æ•°æ®åˆ° data/processed/")
            return None

        # åˆ†å‰²æ–‡æ¡£
        splitter = SentenceSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separator="\n"
        )

        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(
            documents,
            storage_context=storage_context,
            embed_model=self.embed_model,
            transformations=[splitter],
            show_progress=True
        )

        print(f"âœ… ç´¢å¼•åˆ›å»ºå®Œæˆï¼å…± {len(documents)} ä¸ªæ–‡æ¡£")
        return index

    def _load_documents(self) -> List:
        """ä»ç›®å½•åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        if not self.data_dir.exists():
            print(f"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return []

        # è¯»å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        reader = SimpleDirectoryReader(
            str(self.data_dir),
            recursive=True,
            required_exts=[".txt", ".md", ".pdf"]
        )

        documents = reader.load_data()
        print(f"ğŸ“„ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

        return documents

    def query(
        self,
        query_text: str,
        mode: str = "buffett",
        top_k: int = 5,
        similarity_threshold: float = 0.7,
    ) -> Dict:
        """
        æŸ¥è¯¢RAGç³»ç»Ÿ

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            mode: å¯¹è¯æ¨¡å¼ ("buffett" | "munger" | "dual")
            top_k: è¿”å›çš„topæ–‡æ¡£æ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            åŒ…å«å›ç­”å’Œæ¥æºçš„å­—å…¸
        """
        if self.index is None:
            return {
                "answer": "æŠ±æ­‰ï¼Œç³»ç»Ÿè¿˜æ²¡æœ‰æ•°æ®ã€‚è¯·å…ˆæ·»åŠ æ–‡æ¡£åˆ° data/processed/ ç›®å½•ã€‚",
                "sources": []
            }

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = self.index.as_query_engine(
            llm=self.llm,
            similarity_top_k=top_k,
            retrieval_mode="hybrid",  # æ··åˆæ£€ç´¢
            response_mode="compact",
        )

        # æ ¹æ®æ¨¡å¼æ·»åŠ ç³»ç»Ÿæç¤º
        from src.prompts.prompts import get_prompt
        system_prompt = get_prompt(mode, context="{context}")

        # æ‰§è¡ŒæŸ¥è¯¢
        response = query_engine.query(query_text)

        # æå–æ¥æº
        sources = []
        if hasattr(response, "source_nodes"):
            for node in response.source_nodes:
                metadata = node.metadata
                sources.append({
                    "file": metadata.get("file_name", "Unknown"),
                    "score": node.score if hasattr(node, "score") else 0,
                    "text": node.text[:200] + "..." if len(node.text) > 200 else node.text
                })

        return {
            "answer": str(response),
            "sources": sources,
            "mode": mode
        }

    def add_documents(self, file_paths: List[str]):
        """
        æ·»åŠ æ–°æ–‡æ¡£åˆ°ç´¢å¼•

        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # è¿™é‡Œå®ç°å¢é‡æ·»åŠ é€»è¾‘
        # ç®€åŒ–ç‰ˆæœ¬ï¼šé‡æ–°åˆ›å»ºç´¢å¼•
        print("ğŸ“ æ·»åŠ æ–°æ–‡æ¡£...")
        # TODO: å®ç°å¢é‡æ·»åŠ 
        pass

    def chat(
        self,
        message: str,
        history: List[Dict],
        mode: str = "buffett"
    ) -> str:
        """
        èŠå¤©æ¨¡å¼ï¼ˆå¸¦å¯¹è¯å†å²ï¼‰

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: å¯¹è¯å†å²
            mode: å¯¹è¯æ¨¡å¼

        Returns:
            AIå›å¤
        """
        # ç®€å•å®ç°ï¼šåªæŸ¥è¯¢RAGï¼Œæš‚ä¸ä½¿ç”¨å¤æ‚çš„å†å²ç®¡ç†
        # å› ä¸ºå·´è²ç‰¹å’ŒèŠ’æ ¼"è®°æ€§ä¸å¥½"
        result = self.query(message, mode=mode)
        return result["answer"]


# ============= ä¾¿æ·å‡½æ•° =============

def create_rag_system(
    data_dir: str = "./data/processed",
    mode: str = "buffett"
) -> NothingToAddRAG:
    """
    åˆ›å»ºRAGç³»ç»Ÿçš„ä¾¿æ·å‡½æ•°

    Args:
        data_dir: æ•°æ®ç›®å½•
        mode: é»˜è®¤æ¨¡å¼

    Returns:
        RAGç³»ç»Ÿå®ä¾‹
    """
    rag = NothingToAddRAG(data_dir=data_dir)
    return rag


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys

    print("ğŸš€ Nothing to Add RAG System")
    print("=" * 50)

    # åˆå§‹åŒ–
    rag = NothingToAddRAG()

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯ä»·å€¼æŠ•èµ„ï¼Ÿ",
        "å¦‚ä½•è¯„ä¼°ä¸€å®¶å…¬å¸ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æŠ¤åŸæ²³ï¼Ÿ"
    ]

    for query in test_queries:
        print(f"\nâ“ é—®é¢˜: {query}")
        result = rag.query(query, mode="buffett")
        print(f"ğŸ¤– å›ç­”: {result['answer'][:200]}...")
        print(f"ğŸ“š æ¥æº: {len(result['sources'])} ä¸ªæ–‡æ¡£")


# ============= ä½¿ç”¨ç¤ºä¾‹ =============

"""
# åˆå§‹åŒ–
from src.rag.rag_system import NothingToAddRAG

rag = NothingToAddRAG(
    data_dir="./data/processed",
    llm_model="llama3.1"
)

# æŸ¥è¯¢
result = rag.query(
    "ä»€ä¹ˆæ˜¯ä»·å€¼æŠ•èµ„ï¼Ÿ",
    mode="buffett"
)

print(result["answer"])
print("æ¥æº:")
for source in result["sources"]:
    print(f"  - {source['file']} (ç›¸ä¼¼åº¦: {source['score']:.2f})")
"""
